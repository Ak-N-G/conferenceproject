{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ak-N-G/conferenceproject/blob/main/conferenceproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GYlEAB5zxhsx",
      "metadata": {
        "id": "GYlEAB5zxhsx"
      },
      "source": [
        "# Conference Project: Image Classification Models\n",
        "\n",
        "**Models:** Vision Transformer, Swin Transformer, ConvNet, Compressed Vision Transformer\n",
        "\n",
        "**Workflow:** Dataset loading, layer visualization, model training, saving graphs/outputs, evaluating metrics, running on a custom dataset.\n",
        "\n",
        "**Deadline:**\n",
        "- Abstract: 15th August\n",
        "- Paper Completion: 30th August\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dxTemtMxhsy",
      "metadata": {
        "id": "8dxTemtMxhsy"
      },
      "source": [
        "## 1. Environment Setup & Library Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V26GhYcfxhsz",
      "metadata": {
        "id": "V26GhYcfxhsz"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision timm transformers scikit-learn pandas matplotlib seaborn openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7niwIySxhsz",
      "metadata": {
        "id": "d7niwIySxhsz"
      },
      "source": [
        "## 2. Mount Google Drive (for saving weights, graphs, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TpULwJp7xhsz",
      "metadata": {
        "id": "TpULwJp7xhsz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gWlQbNGwxhsz",
      "metadata": {
        "id": "gWlQbNGwxhsz"
      },
      "source": [
        "## 3. Imports & Global Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p67hl51exhsz",
      "metadata": {
        "id": "p67hl51exhsz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Directories\n",
        "RESULTS_DIR = '/content/drive/MyDrive/ConferenceProject/'\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(RESULTS_DIR, 'weights'), exist_ok=True)\n",
        "os.makedirs(os.path.join(RESULTS_DIR, 'graphs'), exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HzC3Dad-xhs0",
      "metadata": {
        "id": "HzC3Dad-xhs0"
      },
      "source": [
        "## 4. Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ImI7r-5dxhs0",
      "metadata": {
        "id": "ImI7r-5dxhs0"
      },
      "outputs": [],
      "source": [
        "# Example: CIFAR-10, replace with your custom dataset if needed\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Required for ViT/Swin\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Class labels\n",
        "CLASSES = train_data.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c82fHMUVxhs0",
      "metadata": {
        "id": "c82fHMUVxhs0"
      },
      "source": [
        "### Visualize Some Sample Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ls_iEoxhs0",
      "metadata": {
        "id": "f7ls_iEoxhs0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "for i, (img, label) in enumerate(train_loader):\n",
        "  if i == 1:\n",
        "    break\n",
        "  for j in range(8):\n",
        "    plt.subplot(2,4,j+1)\n",
        "    plt.imshow((img[j].permute(1,2,0) * 0.5 + 0.5).numpy())\n",
        "    plt.title(CLASSES[label[j]])\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HOMkYvTVxhs0",
      "metadata": {
        "id": "HOMkYvTVxhs0"
      },
      "source": [
        "## 5. Helper Functions (Training, Evaluation, Plotting, Saving)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JanxYBzzxhs0",
      "metadata": {
        "id": "JanxYBzzxhs0"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=10, model_name='model'):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    train_loss, val_loss = [], []\n",
        "    train_acc, val_acc = [], []\n",
        "    best_val_acc = 0.0\n",
        "    best_weights = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss, correct, total = 0, 0, 0\n",
        "        for imgs, labels in tqdm(train_loader):\n",
        "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_loss.append(epoch_loss/len(train_loader))\n",
        "        train_acc.append(correct/total)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss_epoch, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss_epoch += loss.item()\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        val_loss.append(val_loss_epoch/len(val_loader))\n",
        "        val_acc.append(correct/total)\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc[-1] > best_val_acc:\n",
        "            best_val_acc = val_acc[-1]\n",
        "            best_weights = model.state_dict()\n",
        "            torch.save(best_weights, os.path.join(RESULTS_DIR, 'weights', f'{model_name}_best.pt'))\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch+1}: Train Acc={train_acc[-1]:.3f} Val Acc={val_acc[-1]:.3f}\")\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(train_loss, label='Train Loss')\n",
        "    plt.plot(val_loss, label='Val Loss')\n",
        "    plt.legend(); plt.title('Loss')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(train_acc, label='Train Acc')\n",
        "    plt.plot(val_acc, label='Val Acc')\n",
        "    plt.legend(); plt.title('Accuracy')\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, 'graphs', f'{model_name}_loss_acc.png'))\n",
        "    plt.show()\n",
        "    return best_weights\n",
        "\n",
        "def evaluate_and_export(model, data_loader, model_name):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in data_loader:\n",
        "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(imgs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    print(f\"Accuracy={acc:.3f} Precision={prec:.3f} Recall={recall:.3f} F1={f1:.3f}\")\n",
        "\n",
        "    # Export probability matrix to Excel\n",
        "    df = pd.DataFrame(all_probs, columns=CLASSES)\n",
        "    df.to_excel(os.path.join(RESULTS_DIR, f'{model_name}_prob_matrix.xlsx'), index=False)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, 'graphs', f'{model_name}_confusion.png'))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OG4u3fSXxhs0",
      "metadata": {
        "id": "OG4u3fSXxhs0"
      },
      "source": [
        "## 6. Vision Transformer (ViT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RnNn_JRYxhs1",
      "metadata": {
        "id": "RnNn_JRYxhs1"
      },
      "outputs": [],
      "source": [
        "vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=len(CLASSES)).to(DEVICE)\n",
        "best_vit_weights = train_model(vit, train_loader, test_loader, num_epochs=10, model_name='vit')\n",
        "vit.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'weights', 'vit_best.pt')))\n",
        "evaluate_and_export(vit, test_loader, 'vit')\n",
        "# To visualize attention: see timm/transformers/vit docs (optional, advanced)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a-Y584Tjxhs1",
      "metadata": {
        "id": "a-Y584Tjxhs1"
      },
      "source": [
        "## 7. Swin Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zMF5GCZOxhs1",
      "metadata": {
        "id": "zMF5GCZOxhs1"
      },
      "outputs": [],
      "source": [
        "# Reduce batch size for Swin Transformer training on CIFAR-10 to mitigate potential VRAM issues\n",
        "swin = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=len(CLASSES)).to(DEVICE)\n",
        "best_swin_weights = train_model(swin, torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2), torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2), num_epochs=10, model_name='swin')\n",
        "swin.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'weights', 'swin_best.pt')))\n",
        "evaluate_and_export(swin, torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2), 'swin')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D5kl4CMpxhs1",
      "metadata": {
        "id": "D5kl4CMpxhs1"
      },
      "source": [
        "## 8. ConvNet (ResNet as Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c-Vwf_95xhs1",
      "metadata": {
        "id": "c-Vwf_95xhs1"
      },
      "outputs": [],
      "source": [
        "resnet = torchvision.models.resnet18(pretrained=True)\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, len(CLASSES))\n",
        "resnet = resnet.to(DEVICE)\n",
        "best_resnet_weights = train_model(resnet, train_loader, test_loader, num_epochs=10, model_name='resnet')\n",
        "resnet.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'weights', 'resnet_best.pt')))\n",
        "evaluate_and_export(resnet, test_loader, 'resnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JEnb3ye4xhs1",
      "metadata": {
        "id": "JEnb3ye4xhs1"
      },
      "source": [
        "## 9. Compressed Vision Transformer (TinyViT/Slim ViT/Any Efficient Transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afl9mPTpxhs1",
      "metadata": {
        "id": "afl9mPTpxhs1"
      },
      "outputs": [],
      "source": [
        "# Example with a tiny ViT variant (replace with any lighter ViT available)\n",
        "compressed_vit = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=len(CLASSES)).to(DEVICE)\n",
        "best_compressed_weights = train_model(compressed_vit, train_loader, test_loader, num_epochs=10, model_name='compressed_vit')\n",
        "compressed_vit.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'weights', 'compressed_vit_best.pt')))\n",
        "evaluate_and_export(compressed_vit, test_loader, 'compressed_vit')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zl5M_1BUxhs1",
      "metadata": {
        "id": "Zl5M_1BUxhs1"
      },
      "source": [
        "## 10. Visualizing Feature Maps (ConvNet Example)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y5OeEvbvxhs1",
      "metadata": {
        "id": "y5OeEvbvxhs1"
      },
      "outputs": [],
      "source": [
        "def visualize_conv_features(model, img_tensor):\n",
        "    activations = {}\n",
        "    def get_activation(name):\n",
        "        def hook(model, input, output):\n",
        "            activations[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    layer = model.layer1[0]\n",
        "    hook_handle = layer.register_forward_hook(get_activation('conv1'))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _ = model(img_tensor.unsqueeze(0).to(DEVICE))\n",
        "    act = activations['conv1'].cpu()[0]\n",
        "\n",
        "    plt.figure(figsize=(12,6))\n",
        "    for i in range(8):\n",
        "        plt.subplot(2,4,i+1)\n",
        "        plt.imshow(act[i].detach().numpy(), cmap='viridis')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    hook_handle.remove()\n",
        "\n",
        "# Example: Visualize features for one training image\n",
        "sample_img, _ = train_data[0]\n",
        "visualize_conv_features(resnet, sample_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4j-WxhAtxhs1",
      "metadata": {
        "id": "4j-WxhAtxhs1"
      },
      "source": [
        "## 11. Custom Dataset: How to Use Your Own Data\n",
        "\n",
        "Replace the dataset loading section above with code to read images from `/custom_dataset/train/class1/`, `/custom_dataset/val/class1/`, etc. using `datasets.ImageFolder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VU7YNDm-xhs1",
      "metadata": {
        "id": "VU7YNDm-xhs1"
      },
      "outputs": [],
      "source": [
        "# Sample loader for your custom dataset (adjust path/structure as per your folders)\n",
        "CUSTOM_TRAIN = '/content/drive/MyDrive/ConferenceProject/custom_dataset/train'\n",
        "CUSTOM_VAL = '/content/drive/MyDrive/ConferenceProject/custom_dataset/val'\n",
        "train_custom = datasets.ImageFolder(CUSTOM_TRAIN, transform=transform)\n",
        "val_custom = datasets.ImageFolder(CUSTOM_VAL, transform=transform)\n",
        "train_loader_custom = torch.utils.data.DataLoader(train_custom, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader_custom = torch.utils.data.DataLoader(val_custom, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Repeat training/evaluation with custom dataloader, e.g., replacing train_loader, test_loader above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d64fc84",
      "metadata": {
        "id": "5d64fc84"
      },
      "source": [
        "# 12. Using the Animal Image Classification Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70921d82",
      "metadata": {
        "id": "70921d82"
      },
      "outputs": [],
      "source": [
        "# Animal Image Classification Dataset\n",
        "\n",
        "\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"borhanitrash/animal-image-classification-dataset\")\n",
        "\n",
        "print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "# Update the custom dataset paths to the downloaded data\n",
        "CUSTOM_TRAIN = os.path.join(path, 'animal_image_classification_dataset', 'train')\n",
        "CUSTOM_VAL = os.path.join(path, 'animal_image_classification_dataset', 'val')\n",
        "\n",
        "# Verify the updated paths exist\n",
        "if os.path.exists(CUSTOM_TRAIN):\n",
        "    print(f\"Custom train path updated to: {CUSTOM_TRAIN}\")\n",
        "else:\n",
        "    print(f\"Error: Custom train path not found at {CUSTOM_TRAIN}\")\n",
        "\n",
        "if os.path.exists(CUSTOM_VAL):\n",
        "    print(f\"Custom validation path updated to: {CUSTOM_VAL}\")\n",
        "else:\n",
        "    print(f\"Error: Custom validation path not found at {CUSTOM_VAL}\")\n",
        "\n",
        "# Reload the custom datasets with the new paths\n",
        "train_custom = datasets.ImageFolder(CUSTOM_TRAIN, transform=transform)\n",
        "val_custom = datasets.ImageFolder(CUSTOM_VAL, transform=transform)\n",
        "\n",
        "train_loader_custom = torch.utils.data.DataLoader(train_custom, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader_custom = torch.utils.data.DataLoader(val_custom, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Number of training images in custom dataset: {len(train_custom)}\")\n",
        "print(f\"Number of validation images in custom dataset: {len(val_custom)}\")\n",
        "\n",
        "# Update the CLASSES variable based on the custom dataset\n",
        "CLASSES_CUSTOM = train_custom.classes\n",
        "print(f\"Classes in custom dataset: {CLASSES_CUSTOM}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f91bbf89",
      "metadata": {
        "id": "f91bbf89"
      },
      "source": [
        "# 13. Dataset Download (ImageNet from Kaggle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YhwNAkYY6wv-",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YhwNAkYY6wv-"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mayurmadnani/imagenet-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "# Example: Replace with the actual download URL and desired output path\n",
        "!wget -O /content/drive/MyDrive/ImageNet/imagenet_localization_train.tar.gz \"path\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a98a71",
      "metadata": {
        "id": "c2a98a71"
      },
      "outputs": [],
      "source": [
        "# Define the path to your ImageNet dataset on Google Drive\n",
        "IMAGENET_TRAIN_DIR = '/content/drive/MyDrive/path/to/imagenet/train'\n",
        "IMAGENET_VAL_DIR = '/content/drive/MyDrive/path/to/imagenet/val'\n",
        "\n",
        "# Verify the paths exist\n",
        "if not os.path.exists(IMAGENET_TRAIN_DIR):\n",
        "    print(f\"Error: ImageNet training directory not found at {IMAGENET_TRAIN_DIR}\")\n",
        "if not os.path.exists(IMAGENET_VAL_DIR):\n",
        "    print(f\"Error: ImageNet validation directory not found at {IMAGENET_VAL_DIR}\")\n",
        "\n",
        "# Create ImageNet datasets and data loaders\n",
        "# Note: Loading the full ImageNet can take a significant amount of time and memory.\n",
        "# You might need to adjust num_workers or other DataLoader parameters based on your Colab instance.\n",
        "try:\n",
        "    imagenet_train_data = datasets.ImageFolder(IMAGENET_TRAIN_DIR, transform=transform)\n",
        "    imagenet_val_data = datasets.ImageFolder(IMAGENET_VAL_DIR, transform=transform)\n",
        "\n",
        "    imagenet_train_loader = torch.utils.data.DataLoader(imagenet_train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    imagenet_val_loader = torch.utils.data.DataLoader(imagenet_val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Update the CLASSES variable for ImageNet\n",
        "    IMAGENET_CLASSES = imagenet_train_data.classes\n",
        "    print(f\"Number of training images in ImageNet: {len(imagenet_train_data)}\")\n",
        "    print(f\"Number of validation images in ImageNet: {len(imagenet_val_data)}\")\n",
        "    print(f\"Classes in ImageNet: {IMAGENET_CLASSES}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the ImageNet dataset: {e}\")\n",
        "    print(\"Please ensure the paths are correct and the dataset is organized in the specified structure.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71292819",
      "metadata": {
        "id": "71292819"
      },
      "source": [
        "## 14. Vision Transformer (ViT) - Training on ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97ee654f",
      "metadata": {
        "id": "97ee654f"
      },
      "outputs": [],
      "source": [
        "# Initialize Vision Transformer model\n",
        "# You might need to adjust the model size or architecture depending on your resources\n",
        "vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=len(IMAGENET_CLASSES)).to(DEVICE)\n",
        "\n",
        "# Train the ViT model on ImageNet\n",
        "# You might need to adjust num_epochs, learning rate, and other training parameters for ImageNet\n",
        "# Consider adding more frequent checkpointing in train_model for long training\n",
        "best_vit_weights = train_model(vit, imagenet_train_loader, imagenet_val_loader, num_epochs=10, model_name='vit_imagenet')\n",
        "\n",
        "# Load the best weights for evaluation\n",
        "vit.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'weights', 'vit_imagenet_best.pt')))\n",
        "\n",
        "# Evaluate the ViT model on ImageNet\n",
        "# Evaluation on the full ImageNet validation set might take time\n",
        "evaluate_and_export(vit, imagenet_val_loader, 'vit_imagenet')\n",
        "\n",
        "# To visualize attention: see timm/transformers/vit docs (optional, advanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "913f6be2",
      "metadata": {
        "id": "913f6be2"
      },
      "source": [
        "## 15. Swin Transformer - Training on ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "206c0579",
      "metadata": {
        "id": "206c0579"
      },
      "outputs": [],
      "source": [
        "# Initialize Swin Transformer model\n",
        "# You might need to adjust the model size or architecture depending on your resources\n",
        "swin = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=len(IMAGENET_CLASSES)).to(DEVICE)\n",
        "\n",
        "# Train the Swin Transformer model on ImageNet\n",
        "# You might need to adjust num_epochs, learning rate, and other training parameters for ImageNet\n",
        "# Consider adding more frequent checkpointing in train_model for long training\n",
        "# Reduced BATCH_SIZE to 32 to mitigate VRAM issues\n",
        "train_loader_swin = torch.utils.data.DataLoader(imagenet_train_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader_swin = torch.utils.data.DataLoader(imagenet_val_data, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "best_swin_weights = train_model(swin, train_loader_swin, val_loader_swin, num_epochs=10, model_name='swin_imagenet')\n",
        "\n",
        "# Load the best weights for evaluation\n",
        "swin.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'weights', 'swin_imagenet_best.pt')))\n",
        "\n",
        "# Evaluate the Swin Transformer model on ImageNet\n",
        "# Evaluation on the full ImageNet validation set might take time\n",
        "evaluate_and_export(swin, val_loader_swin, 'swin_imagenet')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7558a504",
      "metadata": {
        "id": "7558a504"
      },
      "source": [
        "## 16. ConvNet (ResNet as Example) - Training on ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f7b5b30",
      "metadata": {
        "id": "2f7b5b30"
      },
      "outputs": [],
      "source": [
        "# Initialize ConvNet model (ResNet18 as example)\n",
        "# You might consider a larger ResNet for better performance on ImageNet\n",
        "resnet = torchvision.models.resnet18(pretrained=True)\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, len(IMAGENET_CLASSES))\n",
        "resnet = resnet.to(DEVICE)\n",
        "\n",
        "# Train the ResNet model on ImageNet\n",
        "# You might need to adjust num_epochs, learning rate, and other training parameters for ImageNet\n",
        "# Consider adding more frequent checkpointing in train_model for long training\n",
        "best_resnet_weights = train_model(resnet, imagenet_train_loader, imagenet_val_loader, num_epochs=10, model_name='resnet_imagenet')\n",
        "\n",
        "# Load the best weights for evaluation\n",
        "resnet.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'weights', 'resnet_imagenet_best.pt')))\n",
        "\n",
        "# Evaluate the ResNet model on ImageNet\n",
        "# Evaluation on the full ImageNet validation set might take time\n",
        "evaluate_and_export(resnet, imagenet_val_loader, 'resnet_imagenet')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08fa465c",
      "metadata": {
        "id": "08fa465c"
      },
      "source": [
        "## 17. Compressed Vision Transformer (TinyViT/Slim ViT/Any Efficient Transformer) - Training on ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a2de8c",
      "metadata": {
        "id": "49a2de8c"
      },
      "outputs": [],
      "source": [
        "# Example with a tiny ViT variant (replace with any lighter ViT available)\n",
        "# You might need to adjust the model size or architecture depending on your resources\n",
        "compressed_vit = timm.create_model('vit_tiny_patch16_224', pretrained=True, num_classes=len(IMAGENET_CLASSES)).to(DEVICE)\n",
        "\n",
        "# Train the compressed ViT model on ImageNet\n",
        "# You might need to adjust num_epochs, learning rate, and other training parameters for ImageNet\n",
        "# Consider adding more frequent checkpointing in train_model for long training\n",
        "best_compressed_weights = train_model(compressed_vit, imagenet_train_loader, imagenet_val_loader, num_epochs=10, model_name='compressed_vit_imagenet')\n",
        "\n",
        "# Load the best weights for evaluation\n",
        "compressed_vit.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'weights', 'compressed_vit_imagenet_best.pt')))\n",
        "\n",
        "# Evaluate the compressed ViT model on ImageNet\n",
        "# Evaluation on the full ImageNet validation set might take time\n",
        "evaluate_and_export(compressed_vit, imagenet_val_loader, 'compressed_vit_imagenet')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a707c8",
      "metadata": {
        "id": "a9a707c8"
      },
      "source": [
        "## 18. Visualizing Feature Maps (ConvNet Example) - After ImageNet Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb156c79",
      "metadata": {
        "id": "fb156c79"
      },
      "outputs": [],
      "source": [
        "# Visualize feature maps for the ResNet model trained on ImageNet\n",
        "# Get a sample image from the ImageNet validation dataset\n",
        "if 'imagenet_val_data' in locals() and len(imagenet_val_data) > 0:\n",
        "    sample_img_imagenet, _ = imagenet_val_data[0]\n",
        "    print(\"Visualizing feature maps for the ResNet model trained on ImageNet...\")\n",
        "    visualize_conv_features(resnet, sample_img_imagenet)\n",
        "else:\n",
        "    print(\"ImageNet validation data not available or empty. Cannot visualize feature maps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d74113f",
      "metadata": {
        "id": "2d74113f"
      },
      "source": [
        "# Task\n",
        "Write Python code to train a Swin Transformer model on a TPU, optimized for the TPU architecture. The code should include TPU environment setup, data parallelism using `pytorch/xla`, mixed precision training with `bfloat16`, and adaptations for XLA compatibility."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44f5f595",
      "metadata": {
        "id": "44f5f595"
      },
      "source": [
        "## Tpu environment setup\n",
        "\n",
        "### Subtask:\n",
        "Add code to detect and initialize the TPU environment using `pytorch/xla`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f825c6",
      "metadata": {
        "id": "f5f825c6"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires setting up the TPU environment and initializing it using `pytorch/xla`. This involves importing necessary modules, setting environment variables, and defining a main function to be launched on TPU cores. This can be done in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15a626c",
      "metadata": {
        "id": "a15a626c"
      },
      "outputs": [],
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import os\n",
        "\n",
        "# Define a main training function to be executed on each TPU core\n",
        "def train_tpu(index, args):\n",
        "    device = xm.get_device()\n",
        "    print(f\"TPU is available. Using device: {device}\")\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == '__main__':\n",
        "    # Set environment variables for bfloat16 support and tensor allocation size\n",
        "    os.environ['XLA_USE_BF16'] = '1'\n",
        "    os.environ['XLA_TENSOR_ALLOC_MAXSIZE'] = '100000000'\n",
        "\n",
        "    # Launch the main training function across available TPU cores\n",
        "    xmp.spawn(train_tpu, args=({},), nprocs=8) # Assuming 8 TPU cores are available\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af9e5d1f",
      "metadata": {
        "id": "af9e5d1f"
      },
      "source": [
        "## Data parallelism with xla\n",
        "\n",
        "### Subtask:\n",
        "Modify the data loaders and training loop to distribute the data and model across TPU cores using XLA utilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcad9a8f",
      "metadata": {
        "id": "dcad9a8f"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the existing `train_model` and `evaluate_and_export` functions to work with XLA devices and then updating the main execution block to use these modified functions within `xmp.spawn`. This involves adding a device argument to the functions, moving data to the device, replacing `optimizer.step()` with `xm.optimizer_step`, and moving results back to the CPU for metric calculations. I will implement these changes in the functions first and then show how to use them in the `xmp.spawn` block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4952f4",
      "metadata": {
        "id": "fc4952f4"
      },
      "outputs": [],
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel as pctl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "\n",
        "def train_model_xla(model, train_loader, val_loader, device, num_epochs=10, model_name='model'):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    train_loss, val_loss = [], []\n",
        "    train_acc, val_acc = [], []\n",
        "    best_val_acc = 0.0\n",
        "    best_weights = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss, correct, total = 0, 0, 0\n",
        "        # Wrap the data loader for XLA\n",
        "        para_train_loader = xmp.MpSerialDataLoader(train_loader, device)\n",
        "        for imgs, labels in tqdm(para_train_loader):\n",
        "            imgs, labels = imgs.to(device), labels.to(device) # Move data to XLA device\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            # Use xm.optimizer_step for XLA\n",
        "            xm.optimizer_step(optimizer)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_loss.append(epoch_loss/len(train_loader))\n",
        "        train_acc.append(correct/total)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss_epoch, correct, total = 0, 0, 0\n",
        "        # Wrap the data loader for XLA\n",
        "        para_val_loader = xmp.MpSerialDataLoader(val_loader, device)\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in para_val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device) # Move data to XLA device\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss_epoch += loss.item()\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        val_loss.append(val_loss_epoch/len(val_loader))\n",
        "        val_acc.append(correct/total)\n",
        "\n",
        "        # Save best model (only on master process)\n",
        "        if xm.is_master_ordinal():\n",
        "            if val_acc[-1] > best_val_acc:\n",
        "                best_val_acc = val_acc[-1]\n",
        "                best_weights = model.state_dict()\n",
        "                torch.save(best_weights, os.path.join(RESULTS_DIR, 'weights', f'{model_name}_best.pt'))\n",
        "\n",
        "        scheduler.step()\n",
        "        if xm.is_master_ordinal():\n",
        "             print(f\"Epoch {epoch+1}: Train Acc={train_acc[-1]:.3f} Val Acc={val_acc[-1]:.3f}\")\n",
        "\n",
        "    # Plot (only on master process)\n",
        "    if xm.is_master_ordinal():\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(train_loss, label='Train Loss')\n",
        "        plt.plot(val_loss, label='Val Loss')\n",
        "        plt.legend(); plt.title('Loss')\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(train_acc, label='Train Acc')\n",
        "        plt.plot(val_acc, label='Val Acc')\n",
        "        plt.legend(); plt.title('Accuracy')\n",
        "        plt.savefig(os.path.join(RESULTS_DIR, 'graphs', f'{model_name}_loss_acc.png'))\n",
        "        plt.show()\n",
        "\n",
        "    # Broadcast the best weights to all devices\n",
        "    if best_weights is not None:\n",
        "        best_weights = xm.mesh_reduce('best_weights', best_weights, torch.save)\n",
        "\n",
        "    return best_weights\n",
        "\n",
        "\n",
        "def evaluate_and_export_xla(model, data_loader, device, model_name, classes):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    # Wrap the data loader for XLA\n",
        "    para_data_loader = xmp.MpSerialDataLoader(data_loader, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in para_data_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device) # Move data to XLA device\n",
        "            outputs = model(imgs)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_labels.extend(labels.cpu().numpy()) # Move back to CPU\n",
        "            all_preds.extend(preds.cpu().numpy())   # Move back to CPU\n",
        "            all_probs.extend(probs.cpu().numpy())   # Move back to CPU\n",
        "\n",
        "    # Collect results from all cores and aggregate on master\n",
        "    all_labels = np.concatenate(xm.mesh_reduce('eval_labels', np.array(all_labels), np.concatenate))\n",
        "    all_preds = np.concatenate(xm.mesh_reduce('eval_preds', np.array(all_preds), np.concatenate))\n",
        "    all_probs = np.concatenate(xm.mesh_reduce('eval_probs', np.array(all_probs), np.concatenate))\n",
        "\n",
        "\n",
        "    if xm.is_master_ordinal():\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        prec = precision_score(all_labels, all_preds, average='macro')\n",
        "        recall = recall_score(all_labels, all_preds, average='macro')\n",
        "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "        print(f\"Accuracy={acc:.3f} Precision={prec:.3f} Recall={recall:.3f} F1={f1:.3f}\")\n",
        "\n",
        "        # Export probability matrix to Excel\n",
        "        df = pd.DataFrame(all_probs, columns=classes)\n",
        "        df.to_excel(os.path.join(RESULTS_DIR, f'{model_name}_prob_matrix.xlsx'), index=False)\n",
        "\n",
        "        # Plot Confusion Matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        plt.figure(figsize=(8,6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "        plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')\n",
        "        plt.savefig(os.path.join(RESULTS_DIR, 'graphs', f'{model_name}_confusion.png'))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Main execution block with xmp.spawn\n",
        "def _mp_fn(index, flags):\n",
        "    device = xm.get_device()\n",
        "    print(f\"TPU is available. Using device: {device}\")\n",
        "\n",
        "    # Assuming imagenet_train_data, imagenet_val_data, and IMAGENET_CLASSES are defined in the global scope\n",
        "    # Or passed via the flags dictionary\n",
        "\n",
        "    # Initialize Vision Transformer model on the TPU device\n",
        "    vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=len(IMAGENET_CLASSES)).to(device)\n",
        "\n",
        "    # Create XLA data loaders\n",
        "    imagenet_train_loader_xla = torch.utils.data.DataLoader(imagenet_train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    imagenet_val_loader_xla = torch.utils.data.DataLoader(imagenet_val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "    # Train the ViT model on ImageNet using the XLA-compatible training function\n",
        "    best_vit_weights = train_model_xla(vit, imagenet_train_loader_xla, imagenet_val_loader_xla, device, num_epochs=1, model_name='vit_imagenet_xla') # Reduced epochs for testing\n",
        "\n",
        "    # Load the best weights for evaluation (broadcasted from master)\n",
        "    if best_vit_weights is not None:\n",
        "        vit.load_state_dict(best_vit_weights)\n",
        "    elif xm.is_master_ordinal(): # If training failed on master, try loading from saved file\n",
        "         try:\n",
        "             vit.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'weights', 'vit_imagenet_xla_best.pt')))\n",
        "         except FileNotFoundError:\n",
        "             print(\"Best weights not found on master and broadcasting failed.\")\n",
        "\n",
        "\n",
        "    # Evaluate the ViT model on ImageNet using the XLA-compatible evaluation function\n",
        "    evaluate_and_export_xla(vit, imagenet_val_loader_xla, device, 'vit_imagenet_xla', IMAGENET_CLASSES)\n",
        "\n",
        "# The following block should be executed to run the training on TPU\n",
        "# if __name__ == '__main__':\n",
        "#     os.environ['XLA_USE_BF16'] = '1'\n",
        "#     os.environ['XLA_TENSOR_ALLOC_MAXSIZE'] = '100000000'\n",
        "#\n",
        "#     # You would typically load your dataset here before spawning processes\n",
        "#     # For example:\n",
        "#     # IMAGENET_TRAIN_DIR = '/content/drive/MyDrive/path/to/imagenet/train'\n",
        "#     # IMAGENET_VAL_DIR = '/content/drive/MyDrive/path/to/imagenet/val'\n",
        "#     # imagenet_train_data = datasets.ImageFolder(IMAGENET_TRAIN_DIR, transform=transform)\n",
        "#     # imagenet_val_data = datasets.ImageFolder(IMAGENET_VAL_DIR, transform=transform)\n",
        "#     # IMAGENET_CLASSES = imagenet_train_data.classes\n",
        "#     # BATCH_SIZE = 64 # Define your batch size\n",
        "#     # RESULTS_DIR = '/content/drive/MyDrive/ConferenceProject/' # Define your results directory\n",
        "#\n",
        "#     xmp.spawn(_mp_fn, args=({},), nprocs=8) # Assuming 8 TPU cores are available\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ef79cc8",
      "metadata": {
        "id": "5ef79cc8"
      },
      "source": [
        "## Mixed precision training\n",
        "\n",
        "### Subtask:\n",
        "Implement mixed precision training using `bfloat16` for improved performance on TPUs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "195f700d",
      "metadata": {
        "id": "195f700d"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement mixed precision training using bfloat16 by adding `torch.cuda.amp.autocast(dtype=torch.bfloat16)` to the training and evaluation loops in the `train_model_xla` and `evaluate_and_export_xla` functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0001d47a",
      "metadata": {
        "id": "0001d47a"
      },
      "outputs": [],
      "source": [
        "import torch_xla.amp as xamp # Import for XLA AMP\n",
        "\n",
        "def train_model_xla(model, train_loader, val_loader, device, num_epochs=10, model_name='model'):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    train_loss, val_loss = [], []\n",
        "    train_acc, val_acc = [], []\n",
        "    best_val_acc = 0.0\n",
        "    best_weights = None\n",
        "\n",
        "    scaler = xamp.GradScaler() # Initialize GradScaler for XLA AMP\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss, correct, total = 0, 0, 0\n",
        "        para_train_loader = xmp.MpSerialDataLoader(train_loader, device)\n",
        "        for imgs, labels in tqdm(para_train_loader):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with xamp.autocast(dtype=torch.bfloat16): # Enable bfloat16 autocasting\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward() # Scale the loss and perform backward pass\n",
        "            scaler.step(optimizer) # Update optimizer\n",
        "            scaler.update() # Update scaler\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        train_loss.append(epoch_loss/len(train_loader))\n",
        "        train_acc.append(correct/total)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss_epoch, correct, total = 0, 0, 0\n",
        "        para_val_loader = xmp.MpSerialDataLoader(val_loader, device)\n",
        "        with torch.no_grad():\n",
        "            with xamp.autocast(dtype=torch.bfloat16): # Enable bfloat16 autocasting for validation\n",
        "                for imgs, labels in para_val_loader:\n",
        "                    imgs, labels = imgs.to(device), labels.to(device)\n",
        "                    outputs = model(imgs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss_epoch += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        val_loss.append(val_loss_epoch/len(val_loader))\n",
        "        val_acc.append(correct/total)\n",
        "\n",
        "        if xm.is_master_ordinal():\n",
        "            if val_acc[-1] > best_val_acc:\n",
        "                best_val_acc = val_acc[-1]\n",
        "                best_weights = model.state_dict()\n",
        "                torch.save(best_weights, os.path.join(RESULTS_DIR, 'weights', f'{model_name}_best.pt'))\n",
        "\n",
        "        scheduler.step()\n",
        "        if xm.is_master_ordinal():\n",
        "             print(f\"Epoch {epoch+1}: Train Acc={train_acc[-1]:.3f} Val Acc={val_acc[-1]:.3f}\")\n",
        "\n",
        "    if xm.is_master_ordinal():\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(train_loss, label='Train Loss')\n",
        "        plt.plot(val_loss, label='Val Loss')\n",
        "        plt.legend(); plt.title('Loss')\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(train_acc, label='Train Acc')\n",
        "        plt.plot(val_acc, label='Val Acc')\n",
        "        plt.legend(); plt.title('Accuracy')\n",
        "        plt.savefig(os.path.join(RESULTS_DIR, 'graphs', f'{model_name}_loss_acc.png'))\n",
        "        plt.show()\n",
        "\n",
        "    if best_weights is not None:\n",
        "        best_weights = xm.mesh_reduce('best_weights', best_weights, torch.save)\n",
        "\n",
        "    return best_weights\n",
        "\n",
        "\n",
        "def evaluate_and_export_xla(model, data_loader, device, model_name, classes):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    para_data_loader = xmp.MpSerialDataLoader(data_loader, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with xamp.autocast(dtype=torch.bfloat16): # Enable bfloat16 autocasting for evaluation\n",
        "            for imgs, labels in para_data_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    all_labels = np.concatenate(xm.mesh_reduce('eval_labels', np.array(all_labels), np.concatenate))\n",
        "    all_preds = np.concatenate(xm.mesh_reduce('eval_preds', np.array(all_preds), np.concatenate))\n",
        "    all_probs = np.concatenate(xm.mesh_reduce('eval_probs', np.array(all_probs), np.concatenate))\n",
        "\n",
        "\n",
        "    if xm.is_master_ordinal():\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        prec = precision_score(all_labels, all_preds, average='macro')\n",
        "        recall = recall_score(all_labels, all_preds, average='macro')\n",
        "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "        print(f\"Accuracy={acc:.3f} Precision={prec:.3f} Recall={recall:.3f} F1={f1:.3f}\")\n",
        "\n",
        "        df = pd.DataFrame(all_probs, columns=classes)\n",
        "        df.to_excel(os.path.join(RESULTS_DIR, f'{model_name}_prob_matrix.xlsx'), index=False)\n",
        "\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        plt.figure(figsize=(8,6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "        plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')\n",
        "        plt.savefig(os.path.join(RESULTS_DIR, 'graphs', f'{model_name}_confusion.png'))\n",
        "        plt.show()\n",
        "\n",
        "# The following block should be executed to run the training on TPU\n",
        "# def _mp_fn(index, flags):\n",
        "#     device = xm.get_device()\n",
        "#     print(f\"TPU is available. Using device: {device}\")\n",
        "#\n",
        "#     # Load dataset and define loaders and classes here\n",
        "#     # ...\n",
        "#\n",
        "#     vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=len(IMAGENET_CLASSES)).to(device)\n",
        "#\n",
        "#     imagenet_train_loader_xla = torch.utils.data.DataLoader(imagenet_train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "#     imagenet_val_loader_xla = torch.utils.data.DataLoader(imagenet_val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "#\n",
        "#     best_vit_weights = train_model_xla(vit, imagenet_train_loader_xla, imagenet_val_loader_xla, device, num_epochs=1, model_name='vit_imagenet_xla')\n",
        "#\n",
        "#     if best_vit_weights is not None:\n",
        "#         vit.load_state_dict(best_vit_weights)\n",
        "#     elif xm.is_master_ordinal():\n",
        "#          try:\n",
        "#              vit.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'weights', 'vit_imagenet_xla_best.pt')))\n",
        "#          except FileNotFoundError:\n",
        "#              print(\"Best weights not found on master and broadcasting failed.\")\n",
        "#\n",
        "#     evaluate_and_export_xla(vit, imagenet_val_loader_xla, device, 'vit_imagenet_xla', IMAGENET_CLASSES)\n",
        "#\n",
        "# # if __name__ == '__main__':\n",
        "# #     os.environ['XLA_USE_BF16'] = '1'\n",
        "# #     os.environ['XLA_TENSOR_ALLOC_MAXSIZE'] = '100000000'\n",
        "# #     xmp.spawn(_mp_fn, args=({},), nprocs=8)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}